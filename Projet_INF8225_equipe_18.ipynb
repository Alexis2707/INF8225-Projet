{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        "# Project INF8225 2025, Machine translation: Comparison performance between encoder decoder architecture and decoder only\n",
        "\n",
        "\n",
        "For this project we are going to reuse the overall structure of TP3 where we implemented the encoder decoder transformer and add the implementation of our decoder only architecture\n",
        "\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLbVbH4lu4J0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hxQLxOjRb1KY",
        "outputId": "62d22ca6-e4b1-4642-f36a-b3e9ceede559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch/\n",
            "Collecting torch==2.1.2+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.2%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m473.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu121) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.2+cu121)\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2+cu121) (1.3.0)\n",
            "Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.2+cu121 triton-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "e361bb3d809e47658f500b476df8adb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.16.2\n",
            "  Downloading torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting scipy==1.9.3\n",
            "  Downloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy\n",
            "  Downloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting tqdm (from torchtext==0.16.2)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m247.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from torchtext==0.16.2)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting torch==2.1.2 (from torchtext==0.16.2)\n",
            "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.16.2)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting joblib>=1.0.0 (from scikit-learn==1.1.3)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.1.3)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting urllib3>=1.25 (from torchdata==0.7.1->torchtext==0.16.2)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m225.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools (from spacy)\n",
            "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging>=20.0 (from spacy)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting platformdirs (from wandb)\n",
            "  Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
            "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting psutil>=5.0.0 (from wandb)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting pyyaml (from wandb)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting six>=1.4.0 (from docker-pycreds>=0.4.0->wandb)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.16.2)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchtext==0.16.2)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchtext==0.16.2)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.2->torchtext==0.16.2)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m255.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.0/32.0 MB\u001b[0m \u001b[31m280.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m202.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m217.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m184.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m222.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m224.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m227.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m225.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m245.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m209.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m288.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m341.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m177.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m215.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m227.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m223.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m253.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m279.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m257.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m268.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m260.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m231.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.8/340.8 kB\u001b[0m \u001b[31m297.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m174.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m224.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m244.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m249.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m283.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
            "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m308.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Downloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m306.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m221.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m307.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m303.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m244.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m180.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m296.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m239.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m321.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m221.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m305.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m340.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m232.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m295.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m257.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m329.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m307.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m272.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m323.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: mpmath, cymem, wrapt, wasabi, urllib3, typing-extensions, tqdm, torchinfo, threadpoolctl, sympy, spacy-loggers, spacy-legacy, smmap, six, shellingham, setuptools, setproctitle, pyyaml, pygments, psutil, protobuf, platformdirs, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, murmurhash, mdurl, MarkupSafe, joblib, idna, fsspec, filelock, einops, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, typing-inspection, triton, srsly, smart-open, sentry-sdk, scipy, requests, pydantic-core, preshed, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, marisa-trie, jinja2, gitdb, docker-pycreds, blis, scikit-learn, rich, pydantic, nvidia-cusolver-cu12, language-data, gitpython, wandb, typer, torch, langcodes, confection, weasel, torchdata, thinc, torchtext, spacy\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.11\n",
            "    Uninstalling cymem-2.0.11:\n",
            "      Successfully uninstalled cymem-2.0.11\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.5\n",
            "    Uninstalling spacy-loggers-1.0.5:\n",
            "      Successfully uninstalled spacy-loggers-1.0.5\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.12\n",
            "    Uninstalling spacy-legacy-3.0.12:\n",
            "      Successfully uninstalled spacy-legacy-3.0.12\n",
            "  Attempting uninstall: smmap\n",
            "    Found existing installation: smmap 5.0.2\n",
            "    Uninstalling smmap-5.0.2:\n",
            "      Successfully uninstalled smmap-5.0.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: shellingham\n",
            "    Found existing installation: shellingham 1.5.4\n",
            "    Uninstalling shellingham-1.5.4:\n",
            "      Successfully uninstalled shellingham-1.5.4\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: setproctitle\n",
            "    Found existing installation: setproctitle 1.3.5\n",
            "    Uninstalling setproctitle-1.3.5:\n",
            "      Successfully uninstalled setproctitle-1.3.5\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.18.0\n",
            "    Uninstalling Pygments-2.18.0:\n",
            "      Successfully uninstalled Pygments-2.18.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.3.7\n",
            "    Uninstalling platformdirs-4.3.7:\n",
            "      Successfully uninstalled platformdirs-4.3.7\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.12\n",
            "    Uninstalling murmurhash-1.0.12:\n",
            "      Successfully uninstalled murmurhash-1.0.12\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.21.0\n",
            "    Uninstalling cloudpathlib-0.21.0:\n",
            "      Successfully uninstalled cloudpathlib-0.21.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.10\n",
            "    Uninstalling catalogue-2.0.10:\n",
            "      Successfully uninstalled catalogue-2.0.10\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.0\n",
            "    Uninstalling typing-inspection-0.4.0:\n",
            "      Successfully uninstalled typing-inspection-0.4.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.5.1\n",
            "    Uninstalling srsly-2.5.1:\n",
            "      Successfully uninstalled srsly-2.5.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: sentry-sdk\n",
            "    Found existing installation: sentry-sdk 2.26.1\n",
            "    Uninstalling sentry-sdk-2.26.1:\n",
            "      Successfully uninstalled sentry-sdk-2.26.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.9\n",
            "    Uninstalling preshed-3.0.9:\n",
            "      Successfully uninstalled preshed-3.0.9\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: marisa-trie\n",
            "    Found existing installation: marisa-trie 1.2.1\n",
            "    Uninstalling marisa-trie-1.2.1:\n",
            "      Successfully uninstalled marisa-trie-1.2.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: gitdb\n",
            "    Found existing installation: gitdb 4.0.12\n",
            "    Uninstalling gitdb-4.0.12:\n",
            "      Successfully uninstalled gitdb-4.0.12\n",
            "  Attempting uninstall: docker-pycreds\n",
            "    Found existing installation: docker-pycreds 0.4.0\n",
            "    Uninstalling docker-pycreds-0.4.0:\n",
            "      Successfully uninstalled docker-pycreds-0.4.0\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: language-data\n",
            "    Found existing installation: language_data 1.3.0\n",
            "    Uninstalling language_data-1.3.0:\n",
            "      Successfully uninstalled language_data-1.3.0\n",
            "  Attempting uninstall: gitpython\n",
            "    Found existing installation: GitPython 3.1.44\n",
            "    Uninstalling GitPython-3.1.44:\n",
            "      Successfully uninstalled GitPython-3.1.44\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.9\n",
            "    Uninstalling wandb-0.19.9:\n",
            "      Successfully uninstalled wandb-0.19.9\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2+cu121\n",
            "    Uninstalling torch-2.1.2+cu121:\n",
            "      Successfully uninstalled torch-2.1.2+cu121\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.5.0\n",
            "    Uninstalling langcodes-3.5.0:\n",
            "      Successfully uninstalled langcodes-3.5.0\n",
            "  Attempting uninstall: confection\n",
            "    Found existing installation: confection 0.1.5\n",
            "    Uninstalling confection-0.1.5:\n",
            "      Successfully uninstalled confection-0.1.5\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.5\n",
            "    Uninstalling spacy-3.8.5:\n",
            "      Successfully uninstalled spacy-3.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.30.2 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.9.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "langchain-core 0.3.55 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.9.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.9.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.9.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, but you have scipy 1.9.3 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.1 catalogue-2.0.10 certifi-2025.4.26 charset-normalizer-3.4.1 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 docker-pycreds-0.4.0 einops-0.8.1 filelock-3.18.0 fsspec-2025.3.2 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 jinja2-3.1.6 joblib-1.4.2 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 murmurhash-1.0.12 networkx-3.4.2 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 packaging-25.0 platformdirs-4.3.7 preshed-3.0.9 protobuf-6.30.2 psutil-7.0.0 pydantic-2.11.3 pydantic-core-2.33.1 pygments-2.19.1 pyyaml-6.0.2 requests-2.32.3 rich-14.0.0 scikit-learn-1.1.3 scipy-1.9.3 sentry-sdk-2.27.0 setproctitle-1.3.5 setuptools-79.0.1 shellingham-1.5.4 six-1.17.0 smart-open-7.1.0 smmap-5.0.2 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.13.3 thinc-8.3.4 threadpoolctl-3.6.0 torch-2.1.2 torchdata-0.7.1 torchinfo-1.8.0 torchtext-0.16.2 tqdm-4.67.1 triton-2.1.0 typer-0.15.2 typing-extensions-4.13.2 typing-inspection-0.4.0 urllib3-2.4.0 wandb-0.19.10 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "charset_normalizer",
                  "joblib",
                  "numpy",
                  "psutil",
                  "six",
                  "sklearn",
                  "torch",
                  "torchgen",
                  "tqdm"
                ]
              },
              "id": "195f9bae64224d0095ef3a0511af440a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP\n",
        "# !pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir\n",
        "# !pip install torchtext==0.16.2 --force-reinstall --no-cache-dir\n",
        "# !pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "# !pip install scikit-learn==1.1.3 --force-reinstall --no-cache-dir\n",
        "# !pip install scipy==1.9.3 --force-reinstall --no-cache-dir\n",
        "# !pip install spacy einops wandb torchinfo\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download fr_core_news_sm\n",
        "\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/\n",
        "!pip install torchtext==0.16.2 numpy==1.23.5 scikit-learn==1.1.3 scipy==1.9.3 spacy einops wandb torchinfo --force-reinstall --no-cache-dir\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJQfREvFUdoz",
        "outputId": "4264f66b-f605-4635-956c-23f286daa657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n"
          ]
        }
      ],
      "source": [
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "# cpal\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchtext\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "import spacy\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxNpMbkvUfGE",
        "outputId": "79963cf0-b875-48d6-8b3b-3993bd7b74c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 02:25:16--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  21.6MB/s    in 0.4s    \n",
            "\n",
            "2025-04-27 02:25:17 (21.6 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "209462\n"
          ]
        }
      ],
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "en_nlp = spacy.load('en_core_web_sm')\n",
        "fr_nlp = spacy.load('fr_core_news_sm')\n",
        "def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in en_nlp.tokenizer(text)]\n",
        "def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in fr_nlp.tokenizer(text)]\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppPj9CrnsSoW"
      },
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "\n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "# Models architecture\n",
        "\n",
        "## Tranformer encoder-decoder\n",
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcGlRnZvOnY"
      },
      "source": [
        "## Transformer models\n",
        "Here you have to code the Full Transformer and Decoder-Only Transformer architectures.\n",
        "It is divided in three parts:\n",
        "* Attention layers (done individually)\n",
        "* Encoder and decoder layers (done individually)\n",
        "* Full Transformer: gather the encoder and decoder layers (done individually)\n",
        "\n",
        "The Transformer (or \"Full Transformer\") is presented in the paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaVFTTUlYwd"
      },
      "source": [
        "### Positional Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lIqHye2Vl3gk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    This PE module comes from:\n",
        "    Pytorch. (2021). LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1).to(DEVICE)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).to(DEVICE)\n",
        "        pe = torch.zeros(max_len, 1, d_model).to(DEVICE)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = rearrange(x, \"b s e -> s b e\")\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x = rearrange(x, \"s b e -> b s e\")\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFxV-6M3402p"
      },
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "outputs": [],
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention between the keys and the queries.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "    \"\"\"This code is inspired from http://nlp.seas.harvard.edu/annotated-transformer/#prelims\"\"\"\n",
        "    # On récupère la dimension de chaque tête d'attention\n",
        "    d_k = q.shape[-1]\n",
        "\n",
        "    # Calcul du produit scalaire entre les requêtes (q) et les clés (k)\n",
        "    # On normalise ensuite en divisant par la racine carrée de d_k pour stabiliser l'entraînement\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # Si un masque est fourni, on met à -inf les positions à masquer (empêche l'attention sur ces positions)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    # On applique une softmax pour obtenir des poids d'attention normalisés entre 0 et 1\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # Si un dropout est spécifié, on l'applique aux poids d'attention pour régulariser\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "\n",
        "    # On applique les poids d'attention sur les valeurs (v) pour obtenir la sortie pondérée\n",
        "    y = torch.matmul(attn, v)\n",
        "\n",
        "    # On retourne la sortie finale et les poids d'attention utilisés\n",
        "    return y, attn\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.dim_head = dim // n_heads  # Dimension per head\n",
        "\n",
        "        # Projection layers\n",
        "        self.w_q = nn.Linear(dim, dim, bias=False, device=DEVICE)\n",
        "        self.w_k = nn.Linear(dim, dim, bias=False, device=DEVICE)\n",
        "        self.w_v = nn.Linear(dim, dim, bias=False, device=DEVICE)\n",
        "        self.w_o = nn.Linear(dim, dim, bias=False, device=DEVICE)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        \"\"\"This code is inspired from https://github.com/harvardnlp/annotated-transformer/blob/debc9fd747bb2123160a98046ad1c2d4da44a567/AnnotatedTransformer.ipynb\"\"\"\n",
        "\n",
        "        # On applique les transformations linéaires pour obtenir les requêtes (q), clés (k) et valeurs (v)\n",
        "        q = self.w_q(q)\n",
        "        k = self.w_k(k)\n",
        "        v = self.w_v(v)\n",
        "\n",
        "        # On réarrange les dimensions pour séparer les différentes têtes d'attention\n",
        "        # Avant : [batch_size, seq_len, dim_model]\n",
        "        # Après  : [batch_size, n_heads, seq_len, dim_head]\n",
        "        q = rearrange(q, \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "        k = rearrange(k, \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "        v = rearrange(v, \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "\n",
        "        # On calcule l'attention entre les requêtes et les clés\n",
        "        y, attn_weights = attention(q, k, v, mask=attn_mask, dropout=self.dropout)\n",
        "\n",
        "        # On réarrange les dimensions pour regrouper les têtes d'attention\n",
        "        # Avant : [batch_size, n_heads, seq_len, dim_head]\n",
        "        # Après  : [batch_size, seq_len, dim_model]\n",
        "        y = rearrange(y, \"b h s d -> b s (h d)\")\n",
        "\n",
        "        # Dernière transformation linéaire pour mélanger les têtes d'attention\n",
        "        return self.w_o(y), attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIpHjOtK47DH"
      },
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention using MultiheadAttention(hand made)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            dim=d_model,\n",
        "            n_heads=nhead,\n",
        "            dropout=dropout\n",
        "            )\n",
        "\n",
        "        # Self-attention using nn.MultiheadAttention\n",
        "        ''' self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model,\n",
        "            num_heads=nhead,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "            ) '''\n",
        "\n",
        "\n",
        "        # Cross-attention using MultiheadAttention\n",
        "        self.cross_attn = MultiheadAttention(\n",
        "            dim=d_model,\n",
        "            n_heads=nhead,\n",
        "            dropout=dropout,\n",
        "            )\n",
        "\n",
        "        # Cross-attention using nn.MultiheadAttention\n",
        "        ''' self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model,\n",
        "            num_heads=nhead,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "            ) '''\n",
        "\n",
        "        # Feedforward network\n",
        "        #Fonction obtenue à l'aide de chatgpt\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, device=DEVICE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, device=DEVICE)\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm_layer1 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "        self.norm_layer2 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "        self.norm_layer3 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Self-attention on the target sequence\n",
        "        ''' self_attn_out, _ = self.self_attn(\n",
        "        query=tgt, key=tgt, value=tgt,\n",
        "        attn_mask=tgt_mask_attn,\n",
        "        key_padding_mask=tgt_key_padding_mask\n",
        "        ) '''\n",
        "        self_attn_out, _ = self.self_attn(\n",
        "        q=tgt, k=tgt, v=tgt,\n",
        "        attn_mask=tgt_mask_attn,\n",
        "        key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "\n",
        "        tgt = tgt + self.dropout_layer(self_attn_out)  # Residual connection\n",
        "        tgt = self.norm_layer1(tgt)  # Normalization\n",
        "\n",
        "        #Cross-attention (target attends to encoded source)\n",
        "        ''' cross_attn_out, _ = self.cross_attn(\n",
        "        query=tgt, key=src, value=src,\n",
        "        key_padding_mask=src_key_padding_mask\n",
        "        ) '''\n",
        "        cross_attn_out, _ = self.cross_attn(\n",
        "        q=tgt, k=src, v=src,\n",
        "        key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        tgt = tgt + self.dropout_layer(cross_attn_out)  # Residual connection\n",
        "        tgt = self.norm_layer2(tgt)  # Normalization\n",
        "\n",
        "        # Feedforward network\n",
        "        ff_out = self.feedforward(tgt)\n",
        "        tgt = tgt + self.dropout_layer(ff_out)  # Residual connection\n",
        "        tgt = self.norm_layer3(tgt)  # Normalization\n",
        "\n",
        "        # Output shape: [batch_size, tgt_seq_len, dim_model]\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layer:int ,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "         # Multiple TransformerDecoderLayer added together\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, d_ff, nhead, dropout) for _ in range(num_decoder_layer)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Iteration through each layer\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(src, tgt, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention (handmade)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            dim=d_model,\n",
        "            n_heads=nhead,\n",
        "            dropout=dropout\n",
        "            )\n",
        "\n",
        "        ''' self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model,\n",
        "            num_heads=nhead,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "            ) '''\n",
        "\n",
        "        # Feedforward network\n",
        "        #Obtenu à l'aide de chatgpt\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, device=DEVICE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, device=DEVICE)\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm_layer1 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "        self.norm_layer2 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply self-attention on the source sequence\n",
        "        attn_output, _ = self.self_attn(src, src, src, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        # Add attention output to the original src (residual connection)\n",
        "        src = src + self.dropout(attn_output)\n",
        "\n",
        "        # Normalize result\n",
        "        src = self.norm_layer1(src)\n",
        "\n",
        "        # Apply a feedforward network\n",
        "        ff_output = self.feedforward(src)\n",
        "\n",
        "        # Add feedforward output (residual connections)\n",
        "        src = src + self.dropout(ff_output)\n",
        "\n",
        "        # Normalize result\n",
        "        src = self.norm_layer2(src)\n",
        "\n",
        "        # Return the final output\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # List of TransformerEncoderLayer\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, dim_feedforward, nheads, dropout) for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, key_padding_mask)\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      },
      "source": [
        "### Transformer\n",
        "This section gathers the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder handmade\n",
        "        self.encoder = TransformerEncoder(\n",
        "            d_model=d_model,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            nheads=nhead,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        #Encoder nn.Encoder\n",
        "        ''' self.encoder = nn.Encoder(\n",
        "            d_model=d_model,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            nheads=nhead,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        ) '''\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = TransformerDecoder(\n",
        "            d_model=d_model,\n",
        "            d_ff=dim_feedforward,\n",
        "            num_decoder_layer=num_decoder_layers,\n",
        "            nhead=nhead,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        #Decoder nn.Decoder\n",
        "        ''' self.decoder = nn.Decoder(\n",
        "            d_model=d_model,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            nheads=nhead,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        ) '''\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Encode source\n",
        "        encoder = self.encoder(src, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Decode using target and source\n",
        "        output = self.decoder(\n",
        "            src=encoder,\n",
        "            tgt=tgt,\n",
        "            tgt_mask_attn=tgt_mask_attn,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(n_tokens_src, dim_embedding, device=DEVICE)\n",
        "        self.tgt_embedding = nn.Embedding(n_tokens_tgt, dim_embedding, device=DEVICE)\n",
        "\n",
        "        # Position encoding\n",
        "        self.position_embedding = PositionalEncoding(dim_embedding, dropout).to(DEVICE)\n",
        "\n",
        "        # Transformer model handmade\n",
        "        self.transformer = Transformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads,\n",
        "            num_encoder_layers=n_layers,\n",
        "            num_decoder_layers=n_layers,\n",
        "            dim_feedforward=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Transformer model nn.Transformer\n",
        "        ''' self.transformer = nn.Transformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads,\n",
        "            num_encoder_layers=n_layers,\n",
        "            num_decoder_layers=n_layers,\n",
        "            dim_feedforward=dim_hidden,\n",
        "            dropout=dropout\n",
        "        ) '''\n",
        "\n",
        "        # Final output projection layer\n",
        "        self.final_out = nn.Linear(dim_embedding, n_tokens_tgt, device=DEVICE)\n",
        "\n",
        "        # Padding indexes\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        # Get token embeddings and add positional encodings\n",
        "        src_emb = self.src_embedding(source)\n",
        "        src_emb = self.position_embedding(src_emb)\n",
        "        tgt_emb = self.tgt_embedding(target)\n",
        "        tgt_emb = self.position_embedding(tgt_emb)\n",
        "\n",
        "        # Create attention masks\n",
        "        tgt_mask = self.generate_causal_mask(target)\n",
        "        src_key_padding_mask, tgt_key_padding_mask = self.generate_key_padding_mask(source, target)\n",
        "\n",
        "        # Pass through Transformer\n",
        "        output = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            tgt_mask_attn=tgt_mask\n",
        "        )\n",
        "\n",
        "        # Convert output to logits over vocabulary\n",
        "        output = self.final_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_causal_mask(\n",
        "            self,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending subsequent tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [seq_len_tgt, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = target.shape[1]\n",
        "\n",
        "        tgt_mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
        "        tgt_mask = torch.triu(tgt_mask, diagonal=1).to(target.device)\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def generate_key_padding_mask(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending padding tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        src_key_padding_mask = source == self.src_pad_idx\n",
        "        tgt_key_padding_mask = target == self.tgt_pad_idx\n",
        "\n",
        "        return src_key_padding_mask, tgt_key_padding_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R_AmlCAKIvr"
      },
      "source": [
        "# Decoder Only Tranformer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d1WT4MIkIH0F"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyLayer(nn.Module):\n",
        "    \"\"\"Single Decoder-Only layer with masked self-attention.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention using MultiheadAttention(hand made)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            dim=d_model,\n",
        "            n_heads=nhead,\n",
        "            dropout=dropout\n",
        "            )\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, device=DEVICE),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, device=DEVICE)\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm_layer1 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "        self.norm_layer2 = nn.LayerNorm(d_model, device=DEVICE)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        seq: torch.FloatTensor,\n",
        "        attn_mask: torch.BoolTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Applies masked self-attention and feedforward network.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            seq: Batch of embedded input tokens.\n",
        "                Shape of [batch_size, seq_len, dim_model].\n",
        "            attn_mask: Causal mask preventing attention to future tokens.\n",
        "                       Shape of [seq_len, seq_len].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded tokens.\n",
        "                Shape of [batch_size, seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Apply self-attention\n",
        "        attn_output, _ = self.self_attn(\n",
        "            q=seq,\n",
        "            k=seq,\n",
        "            v=seq,\n",
        "            attn_mask=attn_mask,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            )\n",
        "\n",
        "        # Add attention output to the original seq (residual connection)\n",
        "        seq = seq + self.dropout(attn_output)\n",
        "        # Normalize result\n",
        "        seq = self.norm_layer1(seq)\n",
        "\n",
        "        # Apply a feedforward network\n",
        "        ff_output = self.feedforward(seq)\n",
        "        # Add feedforward output (residual connections)\n",
        "        seq = seq + self.dropout(ff_output)\n",
        "        # Normalize result\n",
        "        seq = self.norm_layer2(seq)\n",
        "\n",
        "        # Return the final output\n",
        "        return seq\n",
        "\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    \"\"\"Stack of Decoder-Only layers with final normalization.\n",
        "       Acts as the core processing unit, taking embedded inputs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input/output embeddings.\n",
        "        d_ff: Hidden dimension of the feedforward networks.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_decoder_layers: Number of stacked decoder layers.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        nhead: int,\n",
        "        num_decoder_layers: int,\n",
        "        d_ff: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.decoder = DecoderOnlyLayer(\n",
        "            d_model=d_model,\n",
        "            d_ff=d_ff,\n",
        "            nhead=nhead,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Stack of DecoderOnlyLayer instances\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderOnlyLayer(d_model, d_ff, nhead, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        seq: torch.FloatTensor,\n",
        "        attn_mask: torch.BoolTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Processes the sequence through the stack of decoder layers.\n",
        "\n",
        "        Args:\n",
        "            seq: Batch of embedded input tokens (with positional encoding).\n",
        "                 Shape of [batch_size, seq_len, d_model].\n",
        "            attn_mask: Causal mask preventing attention to future tokens.\n",
        "                       Shape of [seq_len, seq_len].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                              Shape of [batch_size, seq_len].\n",
        "\n",
        "        Returns:\n",
        "            output: Processed sequence embeddings.\n",
        "                    Shape of [batch_size, seq_len, d_model].\n",
        "        \"\"\"\n",
        "        output = seq\n",
        "        # Pass through each layer in the stack\n",
        "        for layer in self.layers:\n",
        "            output = layer(\n",
        "                seq=output,\n",
        "                attn_mask=attn_mask,\n",
        "                key_padding_mask=key_padding_mask\n",
        "            )\n",
        "        return output\n",
        "\n",
        "\n",
        "class DecoderOnlyTranslationTransformer(nn.Module):\n",
        "    \"\"\"Decoder-Only Transformer for sequence generation tasks.\n",
        "    Manages token embeddings, positional encoding, mask creation,\n",
        "    and final output projection, using DecoderOnlyTransformer as the core.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_vocab: Number of tokens in the vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings.\n",
        "        dim_hidden: Dimension size of the feedforward layers in the core transformer.\n",
        "        num_layers: Number of layers in the decoder stack.\n",
        "        dropout: Dropout rate.\n",
        "        pad_idx: Padding index value in the vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_tokens_vocab: int,\n",
        "        n_heads: int,\n",
        "        dim_embedding: int,\n",
        "        dim_hidden: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        pad_idx: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.token_embedding = nn.Embedding(n_tokens_vocab, dim_embedding)\n",
        "\n",
        "        # Position encoding\n",
        "        self.position_embedding = PositionalEncoding(dim_embedding, dropout)\n",
        "\n",
        "        self.transformer = DecoderOnlyTransformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads,\n",
        "            num_decoder_layers=num_layers,\n",
        "            d_ff=dim_hidden,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Final output projection layer\n",
        "        self.final_out = nn.Linear(dim_embedding, n_tokens_vocab, device=DEVICE)\n",
        "\n",
        "        # Padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        sequence: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the next token logits based on the input sequence.\"\"\"\n",
        "\n",
        "        # Get token embeddings and add positional encodings\n",
        "        seq_emb = self.token_embedding(sequence)\n",
        "        seq_emb = self.position_embedding(seq_emb)\n",
        "\n",
        "        # Create attention masks\n",
        "        attn_mask = self.generate_causal_mask(sequence)\n",
        "        key_padding_mask = self.generate_key_padding_mask(sequence)\n",
        "\n",
        "        # Passe les embeddings et les masques au DecoderOnlyTransformer\n",
        "        processed_seq = self.transformer(\n",
        "            seq=seq_emb,\n",
        "            attn_mask=attn_mask,\n",
        "            key_padding_mask=key_padding_mask\n",
        "        )\n",
        "\n",
        "        # Convert output to logits over vocabulary\n",
        "        # Utilise la sortie du DecoderOnlyTransformer\n",
        "        output = self.final_out(processed_seq)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate_causal_mask(\n",
        "        self,\n",
        "        sequence: torch.LongTensor,\n",
        "    ) -> torch.BoolTensor:\n",
        "        \"\"\"Generate the mask to prevent attending subsequent tokens (causal mask).\"\"\"\n",
        "        seq_len = sequence.shape[1]\n",
        "        # S'assurer que le masque est créé sur le même device que la séquence\n",
        "        mask = torch.triu(torch.ones((seq_len, seq_len), device=sequence.device, dtype=torch.bool), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    # Peut être changer cette partie la\n",
        "    def generate_key_padding_mask(\n",
        "        self,\n",
        "        sequence: torch.LongTensor,\n",
        "    ) -> torch.BoolTensor:\n",
        "        \"\"\"Generate the mask to prevent attending padding tokens.\"\"\"\n",
        "        mask = (sequence == self.pad_idx)\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql6jv2lAK-nF"
      },
      "source": [
        "# Greedy search\n",
        "\n",
        "One idea to explore once you have your model working is to implement a geedy search to generate a target translation from a trained model and an input source string. The next token will simply be the most probable one. Compare this strategy of decoding with the beam search strategy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "outputs": [],
      "source": [
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    #Version obtenue à l'aide de chatgpt non utilisée\n",
        "    ''' # Tokenize and numericalize the input sentence\n",
        "    src_tokens = [src_vocab[token] for token in src_tokenizer(source)]\n",
        "    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(0).to(device)  # [1, seq_len]\n",
        "\n",
        "    # Prepare initial target input (<sos> token)\n",
        "    sos_token = tgt_vocab['<sos>']\n",
        "    eos_token = tgt_vocab['<eos>']\n",
        "    tgt_tokens = [sos_token]\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_sentence_length):\n",
        "            tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)  # [1, cur_len]\n",
        "\n",
        "            # Generate masks\n",
        "            tgt_mask_attn = model.generate_causal_mask(tgt_tensor)\n",
        "            src_key_padding_mask, tgt_key_padding_mask = model.generate_key_padding_mask(src_tensor, tgt_tensor)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(src_tensor, tgt_tensor, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "\n",
        "            # Select the last token output\n",
        "            next_token = logits[:, -1, :].argmax(dim=-1).item()  # Take token with highest probability\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token == eos_token:\n",
        "                break\n",
        "\n",
        "            tgt_tokens.append(next_token)\n",
        "\n",
        "    # Convert token indices back to words\n",
        "    translated_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_tokens[1:]))  # Ignore <sos>\n",
        "\n",
        "    return translated_sentence '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgGFG-uXue6w"
      },
      "source": [
        "# Beam search\n",
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always chooses the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k² sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "outputs": [],
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "\n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  # [batch_size, n_beams, n_tokens]\n",
        "    beams = beams.unsqueeze(dim=2)  # [batch_size, n_beams, 1]\n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)  # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        config: dict,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations, adapting to model architecture.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        config: Dictionary containing model configuration, including:\n",
        "            'src_vocab', 'tgt_vocab', 'src_tokenizer', 'device', 'model_type',\n",
        "            'tgt_pad_idx' (implicitly used via EOS_IDX).\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences ordered by their likelihood.\n",
        "    \"\"\"\n",
        "    src_vocab = config['src_vocab']\n",
        "    tgt_vocab = config['tgt_vocab']\n",
        "    src_tokenizer = config['src_tokenizer']\n",
        "    device = config['device']\n",
        "    model_type = config['model_type']\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    BOS_IDX = tgt_vocab['<bos>']\n",
        "    PAD_IDX = tgt_vocab['<pad>']\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    src_tokens_list = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = torch.LongTensor(src_vocab(src_tokens_list)).unsqueeze(0).to(device)\n",
        "\n",
        "    tgt_tokens = torch.LongTensor([[BOS_IDX]]).to(device)\n",
        "    log_probs = torch.FloatTensor([0.0]).to(device)\n",
        "\n",
        "    completed_hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _step in range(max_sentence_length):\n",
        "            if tgt_tokens.shape[0] == 0:\n",
        "                break\n",
        "\n",
        "            current_batch_size = tgt_tokens.shape[0]\n",
        "\n",
        "            if model_type == 'encoder-decoder':\n",
        "                src_repeated = src_tokens.repeat(current_batch_size, 1)\n",
        "                logits = model.forward(src_repeated, tgt_tokens)\n",
        "            elif model_type == 'decoder-only':\n",
        "                logits = model.forward(tgt_tokens)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
        "\n",
        "            next_token_log_probs = torch.log_softmax(logits[:, -1, :], dim=-1)\n",
        "\n",
        "            cumulative_log_probs = log_probs.unsqueeze(1) + next_token_log_probs\n",
        "\n",
        "            k = min(beam_width, next_token_log_probs.shape[-1])\n",
        "            topk_log_probs, topk_indices = cumulative_log_probs.topk(k, dim=-1)\n",
        "\n",
        "            all_candidate_log_probs = topk_log_probs.view(-1)\n",
        "            parent_beam_indices = torch.arange(current_batch_size, device=device).unsqueeze(1).expand(-1, k).reshape(-1)\n",
        "            all_candidate_tokens = topk_indices.view(-1)\n",
        "\n",
        "            num_candidates = all_candidate_log_probs.shape[0]\n",
        "            num_to_keep = min(num_candidates, max_target)\n",
        "\n",
        "            overall_top_log_probs, overall_top_indices = all_candidate_log_probs.topk(num_to_keep, dim=0)\n",
        "\n",
        "            selected_parent_beam_indices = parent_beam_indices[overall_top_indices]\n",
        "            selected_tokens = all_candidate_tokens[overall_top_indices]\n",
        "            selected_log_probs = overall_top_log_probs\n",
        "\n",
        "            prev_tgt_tokens = tgt_tokens[selected_parent_beam_indices]\n",
        "            new_tgt_tokens = torch.cat([prev_tgt_tokens, selected_tokens.unsqueeze(1)], dim=1)\n",
        "\n",
        "            is_terminated = (selected_tokens == EOS_IDX)\n",
        "            is_active = ~is_terminated\n",
        "\n",
        "            terminated_indices = torch.nonzero(is_terminated).squeeze(1)\n",
        "            for idx in terminated_indices:\n",
        "                score = selected_log_probs[idx].item()\n",
        "                sequence = new_tgt_tokens[idx]\n",
        "                completed_hypotheses.append((score, sequence))\n",
        "\n",
        "            active_indices = torch.nonzero(is_active).squeeze(1)\n",
        "            if active_indices.shape[0] == 0:\n",
        "                break\n",
        "\n",
        "            tgt_tokens = new_tgt_tokens[active_indices]\n",
        "            log_probs = selected_log_probs[active_indices]\n",
        "\n",
        "            if tgt_tokens.shape[0] > beam_width:\n",
        "                top_active_log_probs, top_active_indices = log_probs.topk(beam_width)\n",
        "                tgt_tokens = tgt_tokens[top_active_indices]\n",
        "                log_probs = top_active_log_probs\n",
        "\n",
        "    if not completed_hypotheses:\n",
        "        completed_hypotheses = [(lp.item(), seq) for lp, seq in zip(log_probs, tgt_tokens)]\n",
        "\n",
        "    completed_hypotheses.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    sentences = []\n",
        "    for score, tgt_sentence_tensor in completed_hypotheses:\n",
        "        tgt_sentence_list = tgt_sentence_tensor.tolist()\n",
        "        if BOS_IDX in tgt_sentence_list:\n",
        "            tgt_sentence_list = tgt_sentence_list[tgt_sentence_list.index(BOS_IDX)+1:]\n",
        "        if EOS_IDX in tgt_sentence_list:\n",
        "            eos_pos = tgt_sentence_list.index(EOS_IDX)\n",
        "            tgt_sentence_list = tgt_sentence_list[:eos_pos]\n",
        "\n",
        "        tgt_sentence_str = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence_list))\n",
        "        sentences.append(tgt_sentence_str)\n",
        "\n",
        "    sentences_with_scores = list(zip(sentences, [h[0] for h in completed_hypotheses]))\n",
        "\n",
        "    return sentences_with_scores[:max_target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)  # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    if config['model_type'] == 'encoder-decoder':\n",
        "        pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    elif config['model_type'] == 'decoder-only':\n",
        "        pred = model(target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "\n",
        "    # pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    # pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[-1])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()  # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "\n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "\n",
        "        # Logs\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "        val_pred, val_prob = beam_search(\n",
        "            model,\n",
        "            val_source,\n",
        "            # config['src_vocab'],\n",
        "            # config['tgt_vocab'],\n",
        "            # config['src_tokenizer'],\n",
        "            # device,  # It can take a lot of VRAM\n",
        "            config=config,\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "\n",
        "        logs = {**train_logs, **val_logs}  # Merge dictionnaries\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        train_pred, train_prob = beam_search(\n",
        "            model,\n",
        "            train_source,\n",
        "            # config['src_vocab'],\n",
        "            # config['tgt_vocab'],\n",
        "            # config['src_tokenizer'],\n",
        "            # device,  # It can take a lot of VRAM\n",
        "            config=config,\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob,\n",
        "            val_source, val_target, val_pred, val_prob,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "\n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YImgxCWjlWni"
      },
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WriScTUEsRHr",
        "outputId": "ec29c228-fc63-48ab-bdf4-bfbe16ad50ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexis-27\u001b[0m (\u001b[33minf8225_equipe_18\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Sun Apr 27 02:26:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Checking GPU and logging to wandb\n",
        "\n",
        "!wandb login\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqmpxnO1lgDy",
        "outputId": "3e8a1551-cb7d-4afb-89ae-dcb4b2d0d7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 2,175\n",
            "French vocabulary size: 2,660\n",
            "\n",
            "Training examples: 145,333\n",
            "Validation examples: 16,067\n"
          ]
        }
      ],
      "source": [
        "# Instanciate the datasets\n",
        "\n",
        "# A tester [5, 10, 20, 50, 100]\n",
        "MAX_SEQ_LEN = 10\n",
        "\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywFEpplOU5dn",
        "outputId": "9012c604-c979-4827-b5b5-ddf0dc401405"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "DecoderOnlyTranslationTransformer             [128, 10, 2660]           --\n",
              "├─Embedding: 1-1                              [128, 10, 196]            521,360\n",
              "├─PositionalEncoding: 1-2                     [128, 10, 196]            --\n",
              "│    └─Dropout: 2-1                           [128, 10, 196]            --\n",
              "├─DecoderOnlyTransformer: 1-3                 [128, 10, 196]            255,252\n",
              "│    └─ModuleList: 2-2                        --                        --\n",
              "│    │    └─DecoderOnlyLayer: 3-1             [128, 10, 196]            255,252\n",
              "│    │    └─DecoderOnlyLayer: 3-2             [128, 10, 196]            255,252\n",
              "│    │    └─DecoderOnlyLayer: 3-3             [128, 10, 196]            255,252\n",
              "├─Linear: 1-4                                 [128, 10, 2660]           524,020\n",
              "===============================================================================================\n",
              "Total params: 2,066,388\n",
              "Trainable params: 2,066,388\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 231.83\n",
              "===============================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 79.26\n",
              "Params size (MB): 7.24\n",
              "Estimated Total Size (MB): 86.51\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "\n",
        "    # A tester [5, 10, 20, 50]\n",
        "    'epochs': 50,\n",
        "\n",
        "    # A tester [32, 64, 128, 512, 1024]\n",
        "    'batch_size': 128,\n",
        "\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'decoder-only', # 'encoder-decoder' ou 'decoder-only'\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "if config['model_type'] == 'encoder-decoder':\n",
        "  model = TranslationTransformer(\n",
        "      config['n_tokens_src'],\n",
        "      config['n_tokens_tgt'],\n",
        "      config['n_heads'],\n",
        "      config['dim_embedding'],\n",
        "      config['dim_hidden'],\n",
        "      config['n_layers'],\n",
        "      config['dropout'],\n",
        "      config['src_pad_idx'],\n",
        "      config['tgt_pad_idx'],\n",
        "  )\n",
        "  summary_input_size = [\n",
        "      (config['batch_size'], config['max_sequence_length']), # src\n",
        "      (config['batch_size'], config['max_sequence_length'])  # tgt\n",
        "  ]\n",
        "  summary_dtypes = [torch.long, torch.long]\n",
        "elif config['model_type'] == 'decoder-only':\n",
        "  model = DecoderOnlyTranslationTransformer(\n",
        "      config['n_tokens_tgt'],\n",
        "      config['n_heads'],\n",
        "      config['dim_embedding'],\n",
        "      config['dim_hidden'],\n",
        "      config['n_layers'],\n",
        "      config['dropout'],\n",
        "      config['tgt_pad_idx'],\n",
        "  )\n",
        "  summary_input_size = [\n",
        "      (config['batch_size'], config['max_sequence_length']), # src\n",
        "  ]\n",
        "  summary_dtypes = [torch.long]\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=summary_input_size,\n",
        "    dtypes=summary_dtypes,\n",
        "    depth=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "maOTVtk4acxD",
        "outputId": "49f48f1a-81e3-4f4d-b8ee-1125895d5e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250427_025224-lkoieh14</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet/runs/lkoieh14' target=\"_blank\">Seq: 10, Epoch: 50, Batch size: 128</a></strong> to <a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet' target=\"_blank\">https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet/runs/lkoieh14' target=\"_blank\">https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet/runs/lkoieh14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 50 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.82     top-1: 0.40    top-5: 0.60    top-10: 0.69\n",
            "Eval -    loss: 2.76     top-1: 0.41    top-5: 0.61    top-10: 0.70\n",
            "I'm in no hurry.\n",
            "je ne peux pas y aller .\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.69     top-1: 0.42    top-5: 0.61    top-10: 0.71\n",
            "Eval -    loss: 2.64     top-1: 0.42    top-5: 0.62    top-10: 0.71\n",
            "They said no.\n",
            "je ne l' ai pas vu .\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.63     top-1: 0.42    top-5: 0.62    top-10: 0.72\n",
            "Eval -    loss: 2.57     top-1: 0.43    top-5: 0.63    top-10: 0.73\n",
            "Everybody was startled.\n",
            "je n' ai rien fait de mal .\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.57     top-1: 0.43    top-5: 0.63    top-10: 0.73\n",
            "Eval -    loss: 2.54     top-1: 0.43    top-5: 0.64    top-10: 0.73\n",
            "I'm sick of you.\n",
            "je n' ai rien vu .\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.53     top-1: 0.43    top-5: 0.64    top-10: 0.74\n",
            "Eval -    loss: 2.50     top-1: 0.43    top-5: 0.64    top-10: 0.74\n",
            "Why do zebras have stripes?\n",
            "je ne peux pas le faire .\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.48     top-1: 0.44    top-5: 0.64    top-10: 0.74\n",
            "Eval -    loss: 2.48     top-1: 0.44    top-5: 0.65    top-10: 0.74\n",
            "That was the idea.\n",
            "je pense que tom a raison .\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.46     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "Eval -    loss: 2.47     top-1: 0.44    top-5: 0.65    top-10: 0.74\n",
            "Lay down on the couch.\n",
            "je n' ai rien vu .\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 2.44     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "Eval -    loss: 2.45     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "Do you mind much?\n",
            "je ne sais pas quoi faire .\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 2.42     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "Eval -    loss: 2.44     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "You're partially responsible.\n",
            "c' est une question difficile .\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 2.41     top-1: 0.44    top-5: 0.65    top-10: 0.76\n",
            "Eval -    loss: 2.43     top-1: 0.44    top-5: 0.65    top-10: 0.75\n",
            "We won.\n",
            "je n' ai pas d' amis .\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 2.39     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "Eval -    loss: 2.43     top-1: 0.45    top-5: 0.65    top-10: 0.75\n",
            "I need to clear my head.\n",
            "je me suis fait mal .\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 2.38     top-1: 0.44    top-5: 0.66    top-10: 0.76\n",
            "Eval -    loss: 2.42     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "My cat loves toys.\n",
            "je n' ai pas d' autre choix .\n",
            "\n",
            "Epoch 13\n",
            "Train -   loss: 2.37     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "Eval -    loss: 2.41     top-1: 0.45    top-5: 0.65    top-10: 0.75\n",
            "South Africa is far away.\n",
            "je n' ai pas de fièvre .\n",
            "\n",
            "Epoch 14\n",
            "Train -   loss: 2.35     top-1: 0.45    top-5: 0.66    top-10: 0.77\n",
            "Eval -    loss: 2.41     top-1: 0.44    top-5: 0.66    top-10: 0.75\n",
            "Don't leave town.\n",
            "je n' ai rien à faire .\n",
            "\n",
            "Epoch 15\n",
            "Train -   loss: 2.36     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "Eval -    loss: 2.41     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "He cannot have said that.\n",
            "c' est un problème .\n",
            "\n",
            "Epoch 16\n",
            "Train -   loss: 2.35     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "Eval -    loss: 2.40     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "Are you happy here?\n",
            "je ne suis pas en colère .\n",
            "\n",
            "Epoch 17\n",
            "Train -   loss: 2.33     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.40     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "This clock gains two minutes a day.\n",
            "je sais que tom est canadien .\n",
            "\n",
            "Epoch 18\n",
            "Train -   loss: 2.32     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.40     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "You must not say it.\n",
            "je veux que vous soyez mon amie .\n",
            "\n",
            "Epoch 19\n",
            "Train -   loss: 2.32     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.39     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "Today isn't my day.\n",
            "je ne suis pas un enfant .\n",
            "\n",
            "Epoch 20\n",
            "Train -   loss: 2.32     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.39     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "They couldn't find the problem.\n",
            "je n' ai rien à faire .\n",
            "\n",
            "Epoch 21\n",
            "Train -   loss: 2.30     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.39     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "I'd never do a thing like that.\n",
            "je ne suis pas ton ami .\n",
            "\n",
            "Epoch 22\n",
            "Train -   loss: 2.30     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "Eval -    loss: 2.38     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "You know everything.\n",
            "je ne veux pas que vous soyez blessé .\n",
            "\n",
            "Epoch 23\n",
            "Train -   loss: 2.30     top-1: 0.46    top-5: 0.68    top-10: 0.77\n",
            "Eval -    loss: 2.38     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "You must be starving.\n",
            "je me suis fait couper les cheveux .\n",
            "\n",
            "Epoch 24\n",
            "Train -   loss: 2.29     top-1: 0.46    top-5: 0.68    top-10: 0.77\n",
            "Eval -    loss: 2.38     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "Put your valuables in the safe.\n",
            "je n' ai rien à perdre .\n",
            "\n",
            "Epoch 25\n",
            "Train -   loss: 2.28     top-1: 0.46    top-5: 0.68    top-10: 0.77\n",
            "Eval -    loss: 2.38     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "I'm only telling the truth.\n",
            "je ne suis pas un héros .\n",
            "\n",
            "Epoch 26\n",
            "Train -   loss: 2.28     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.38     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "Don't give up hope.\n",
            "je ne sais pas qui c' est .\n",
            "\n",
            "Epoch 27\n",
            "Train -   loss: 2.28     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "I don't have the money.\n",
            "je sais que tom est un peu fou .\n",
            "\n",
            "Epoch 28\n",
            "Train -   loss: 2.27     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "Do you know who drank my coffee?\n",
            "je sais que tom est canadien .\n",
            "\n",
            "Epoch 29\n",
            "Train -   loss: 2.26     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "I've got a good idea.\n",
            "je ne suis pas un enfant .\n",
            "\n",
            "Epoch 30\n",
            "Train -   loss: 2.27     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.66    top-10: 0.76\n",
            "This is mine, isn't it?\n",
            "je n' ai pas de petite amie .\n",
            "\n",
            "Epoch 31\n",
            "Train -   loss: 2.25     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "You're very understanding.\n",
            "je n' ai pas le temps .\n",
            "\n",
            "Epoch 32\n",
            "Train -   loss: 2.25     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "We can handle that.\n",
            "je veux que vous me fassiez confiance .\n",
            "\n",
            "Epoch 33\n",
            "Train -   loss: 2.26     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.76\n",
            "Let's hope he's all right.\n",
            "je n' ai rien à manger .\n",
            "\n",
            "Epoch 34\n",
            "Train -   loss: 2.24     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "This plant is edible.\n",
            "je sais que tom est un génie .\n",
            "\n",
            "Epoch 35\n",
            "Train -   loss: 2.23     top-1: 0.46    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "The buses are running late today.\n",
            "je n' ai rien à ajouter .\n",
            "\n",
            "Epoch 36\n",
            "Train -   loss: 2.23     top-1: 0.46    top-5: 0.69    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.45    top-5: 0.67    top-10: 0.77\n",
            "We both teach French.\n",
            "je ne pense pas que ce soit vrai .\n",
            "\n",
            "Epoch 37\n",
            "Train -   loss: 2.23     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "We can't help Tom.\n",
            "je me suis senti trahi .\n",
            "\n",
            "Epoch 38\n",
            "Train -   loss: 2.25     top-1: 0.46    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Slip into this.\n",
            "je n' ai rien à dire .\n",
            "\n",
            "Epoch 39\n",
            "Train -   loss: 2.25     top-1: 0.47    top-5: 0.68    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "That was close.\n",
            "je me suis fait mal .\n",
            "\n",
            "Epoch 40\n",
            "Train -   loss: 2.22     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Do you speak French?\n",
            "je ne suis pas un menteur .\n",
            "\n",
            "Epoch 41\n",
            "Train -   loss: 2.22     top-1: 0.46    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "You have a good memory.\n",
            "je n' ai pas le temps .\n",
            "\n",
            "Epoch 42\n",
            "Train -   loss: 2.23     top-1: 0.46    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Hold the vase in both hands.\n",
            "je n' ai pas le temps .\n",
            "\n",
            "Epoch 43\n",
            "Train -   loss: 2.21     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "You're sleepy.\n",
            "je ne suis pas un enfant .\n",
            "\n",
            "Epoch 44\n",
            "Train -   loss: 2.24     top-1: 0.46    top-5: 0.69    top-10: 0.78\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "This is just what I wanted.\n",
            "je n' ai pas d' arme .\n",
            "\n",
            "Epoch 45\n",
            "Train -   loss: 2.21     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "My girlfriend is a good dancer.\n",
            "je n' ai pas de portable .\n",
            "\n",
            "Epoch 46\n",
            "Train -   loss: 2.21     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Don't be afraid.\n",
            "je n' ai pas le temps .\n",
            "\n",
            "Epoch 47\n",
            "Train -   loss: 2.21     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "Tom described the incident in detail.\n",
            "je sais que tom est un type brillant .\n",
            "\n",
            "Epoch 48\n",
            "Train -   loss: 2.19     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "We have a reservation.\n",
            "je n' ai rien fait de mal .\n",
            "\n",
            "Epoch 49\n",
            "Train -   loss: 2.20     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "This book deals with China.\n",
            "je veux que vous soyez revenu .\n",
            "\n",
            "Epoch 50\n",
            "Train -   loss: 2.21     top-1: 0.47    top-5: 0.69    top-10: 0.79\n",
            "Eval -    loss: 2.36     top-1: 0.46    top-5: 0.67    top-10: 0.77\n",
            "I don't have to clean my room.\n",
            "je ne pense pas que ce soit correct .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▇▆▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▁▃▃▂▂▂▂▃▃▂▂▂▂▂▂</td></tr><tr><td>Train - top-1</td><td>▁▂▄▄▄▅█▅▅▆▆▆▆▆▆▆▆▇▇▆▆▆▆▆▇▇▇█▇▇▇▇██▇█▇███</td></tr><tr><td>Train - top-10</td><td>▁▁▃▄▄▄▄▅▅▅▆▅▆▆▆▆▆▆▆▆▆▆▇▆▇▆▆▇▆▇▆▇▇█▇▇▇▇▇▇</td></tr><tr><td>Train - top-5</td><td>▁▂▂▂▄▄▅▅▅▅▆▅▆▅▅▅▆▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>Validation - loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▄▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇█▇████████████████</td></tr><tr><td>Validation - top-10</td><td>▁▂▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇█▇██████████████</td></tr><tr><td>Validation - top-5</td><td>▁▂▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇▇█▇███████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.20973</td></tr><tr><td>Train - top-1</td><td>0.46735</td></tr><tr><td>Train - top-10</td><td>0.78954</td></tr><tr><td>Train - top-5</td><td>0.69037</td></tr><tr><td>Validation - loss</td><td>2.3594</td></tr><tr><td>Validation - top-1</td><td>0.45663</td></tr><tr><td>Validation - top-10</td><td>0.7669</td></tr><tr><td>Validation - top-5</td><td>0.66738</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Seq: 10, Epoch: 50, Batch size: 128</strong> at: <a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet/runs/lkoieh14' target=\"_blank\">https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet/runs/lkoieh14</a><br> View project at: <a href='https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet' target=\"_blank\">https://wandb.ai/inf8225_equipe_18/INF8225%20-%20Projet</a><br>Synced 5 W&B file(s), 1 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250427_025224-lkoieh14/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online  # online / offline / disabled to activate, deactivate or turn off WandB logging\n",
        "# !wandb offline\n",
        "\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - Projet',  # Title of your project\n",
        "        group='Transformer Decoder-only',  # In what group of runs do you want this run to be in?\n",
        "        save_code=True,\n",
        "        name='Seq: 10, Epoch: 50, Batch size: 128'\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PFIyvKUefdV",
        "outputId": "828a6b74-3d74-4609-a5dc-7c47d4289a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. (0.52463%) \t il est impossible de trouver votre travail ici.\n",
            "1. (0.35067%) \t il est impossible de trouver ton travail ici.\n",
            "2. (0.26617%) \t il est impossible de travailler votre travail ici.\n",
            "3. (0.25073%) \t il est impossible de tenter votre travail ici.\n",
            "4. (0.22054%) \t il est impossible de faire votre travail ici.\n"
          ]
        }
      ],
      "source": [
        "sentence = \"It is possible to try your work here.\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:5]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6-OpYG_hz2R"
      },
      "source": [
        "---\n",
        "#Understanding the Architecture of a Decoder-Only Transformer: what inspired our project\n",
        "\n",
        "Sources:\n",
        "\n",
        "[this blog post](https://medium.com/international-school-of-ai-data-science/building-custom-gpt-with-pytorch-59e5ba8102d4). The [first \"GPT\" paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), and the paper cited by this GPT-1 paper for the Decoder Only architecture used for GPT, [i.e. this paper](https://arxiv.org/abs/1801.10198)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZwaVFTTUlYwd",
        "OFxV-6M3402p",
        "nIpHjOtK47DH",
        "ql6jv2lAK-nF",
        "LgGFG-uXue6w",
        "WVr2FuDcZxC6"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}